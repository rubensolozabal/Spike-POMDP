{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solozabal/miniforge3/envs/pomdp/lib/python3.8/site-packages/gym/envs/registration.py:423: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
      "  logger.warn(\n",
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "/home/solozabal/miniforge3/envs/pomdp/lib/python3.8/site-packages/gym/utils/env_checker.py:200: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n",
      "/home/solozabal/miniforge3/envs/pomdp/lib/python3.8/site-packages/gym/envs/registration.py:568: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/solozabal/miniforge3/envs/pomdp/lib/python3.8/site-packages/gym/envs/registration.py:407: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
      "  logger.warn(\n",
      "pybullet build time: Apr 19 2024 09:51:31\n",
      "/home/solozabal/miniforge3/envs/pomdp/lib/python3.8/site-packages/gym/spaces/box.py:112: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/solozabal/miniforge3/envs/pomdp/lib/python3.8/site-packages/gym/utils/env_checker.py:144: UserWarning: \u001b[33mWARN: Agent's minimum observation space value is -infinity. This is probably too low.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/solozabal/miniforge3/envs/pomdp/lib/python3.8/site-packages/gym/utils/env_checker.py:148: UserWarning: \u001b[33mWARN: Agent's maxmimum observation space value is infinity. This is probably too high\u001b[0m\n",
      "  logger.warn(\n",
      "/home/solozabal/miniforge3/envs/pomdp/lib/python3.8/site-packages/gym/envs/registration.py:619: UserWarning: \u001b[33mWARN: Env check failed with the following message: The environment cannot be reset with a random seed, even though `seed` or `kwargs` appear in the signature. This should never happen, please report this issue. The error was: reset() got an unexpected keyword argument 'seed'\n",
      "You can set `disable_env_checker=True` to disable this check.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n",
      "argv[0]=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solozabal/Documents/projects/rl-spike/pomdp-baselines/utils/logger.py:9: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import OrderedDict, Set\n",
      "/home/solozabal/miniforge3/envs/pomdp/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# import environments\n",
    "import envs.pomdp\n",
    "\n",
    "# import recurrent model-free RL (separate architecture)\n",
    "from policies.models.policy_rnn import ModelFreeOffPolicy_Separate_RNN as Policy_RNN\n",
    "\n",
    "# import the replay buffer\n",
    "from buffers.seq_replay_buffer_vanilla import SeqReplayBuffer\n",
    "from utils import helpers as utl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchkit.pytorch_utils as ptu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a POMDP environment: Pendulum-V (only observe the velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<OrderEnforcing<POMDPWrapper<TimeLimit<OrderEnforcing<PendulumEnv<Pendulum-V-v0>>>>>>> 1 1 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/solozabal/miniforge3/envs/pomdp/lib/python3.8/site-packages/gym/utils/env_checker.py:200: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "cuda_id = 0  # -1 if using cpu\n",
    "ptu.set_gpu_mode(torch.cuda.is_available() and cuda_id >= 0, cuda_id)\n",
    "\n",
    "env_name = \"Pendulum-V-v0\"\n",
    "env = gym.make(env_name)\n",
    "max_trajectory_len = env._max_episode_steps\n",
    "act_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "print(env, obs_dim, act_dim, max_trajectory_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurent model-free RL agent: separate architecture, `lstm` encoder, `oar` policy input space, `td3` RL algorithm (context length set later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Policy_RNN(\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=act_dim,\n",
    "    encoder=\"lstm\",\n",
    "    algo_name=\"td3\",\n",
    "    action_embedding_size=8,\n",
    "    observ_embedding_size=32,\n",
    "    reward_embedding_size=8,\n",
    "    rnn_hidden_size=128,\n",
    "    dqn_layers=[128, 128],\n",
    "    policy_layers=[128, 128],\n",
    "    lr=0.0003,\n",
    "    gamma=0.9,\n",
    "    tau=0.005,\n",
    ").to(ptu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define other training parameters such as context length and training frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total env episodes 155 total env steps 31000\n"
     ]
    }
   ],
   "source": [
    "num_updates_per_iter = 1.0  # training frequency\n",
    "sampled_seq_len = 64  # context length\n",
    "buffer_size = 1e6\n",
    "batch_size = 32\n",
    "\n",
    "num_iters = 150\n",
    "num_init_rollouts_pool = 5\n",
    "num_rollouts_per_iter = 1\n",
    "total_rollouts = num_init_rollouts_pool + num_iters * num_rollouts_per_iter\n",
    "n_env_steps_total = max_trajectory_len * total_rollouts\n",
    "_n_env_steps_total = 0\n",
    "print(\"total env episodes\", total_rollouts, \"total env steps\", n_env_steps_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define key functions: collect rollouts and policy update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def collect_rollouts(\n",
    "    num_rollouts, random_actions=False, deterministic=False, train_mode=True\n",
    "):\n",
    "    \"\"\"collect num_rollouts of trajectories in task and save into policy buffer\n",
    "    :param\n",
    "        random_actions: whether to use policy to sample actions, or randomly sample action space\n",
    "        deterministic: deterministic action selection?\n",
    "        train_mode: whether to train (stored to buffer) or test\n",
    "    \"\"\"\n",
    "    if not train_mode:\n",
    "        assert random_actions == False and deterministic == True\n",
    "\n",
    "    total_steps = 0\n",
    "    total_rewards = 0.0\n",
    "\n",
    "    for idx in range(num_rollouts):\n",
    "        steps = 0\n",
    "        rewards = 0.0\n",
    "        obs = ptu.from_numpy(env.reset())\n",
    "        obs = obs.reshape(1, obs.shape[-1])\n",
    "        done_rollout = False\n",
    "\n",
    "        # get hidden state at timestep=0, None for mlp\n",
    "        action, reward, internal_state = agent.get_initial_info()\n",
    "\n",
    "        if train_mode:\n",
    "            # temporary storage\n",
    "            obs_list, act_list, rew_list, next_obs_list, term_list = (\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "            )\n",
    "\n",
    "        while not done_rollout:\n",
    "            if random_actions:\n",
    "                action = ptu.FloatTensor([env.action_space.sample()])  # (1, A)\n",
    "            else:\n",
    "                # policy takes hidden state as input for rnn, while takes obs for mlp\n",
    "                (action, _, _, _), internal_state = agent.act(\n",
    "                    prev_internal_state=internal_state,\n",
    "                    prev_action=action,\n",
    "                    reward=reward,\n",
    "                    obs=obs,\n",
    "                    deterministic=deterministic,\n",
    "                )\n",
    "            # observe reward and next obs (B=1, dim)\n",
    "            next_obs, reward, done, info = utl.env_step(env, action.squeeze(dim=0))\n",
    "            done_rollout = False if ptu.get_numpy(done[0][0]) == 0.0 else True\n",
    "\n",
    "            # update statistics\n",
    "            steps += 1\n",
    "            rewards += reward.item()\n",
    "\n",
    "            # early stopping env: such as rmdp, pomdp, generalize tasks. term ignores timeout\n",
    "            term = (\n",
    "                False\n",
    "                if \"TimeLimit.truncated\" in info or steps >= max_trajectory_len\n",
    "                else done_rollout\n",
    "            )\n",
    "\n",
    "            if train_mode:\n",
    "                # append tensors to temporary storage\n",
    "                obs_list.append(obs)  # (1, dim)\n",
    "                act_list.append(action)  # (1, dim)\n",
    "                rew_list.append(reward)  # (1, dim)\n",
    "                term_list.append(term)  # bool\n",
    "                next_obs_list.append(next_obs)  # (1, dim)\n",
    "\n",
    "            # set: obs <- next_obs\n",
    "            obs = next_obs.clone()\n",
    "\n",
    "        if train_mode:\n",
    "            # add collected sequence to buffer\n",
    "            policy_storage.add_episode(\n",
    "                observations=ptu.get_numpy(torch.cat(obs_list, dim=0)),  # (L, dim)\n",
    "                actions=ptu.get_numpy(torch.cat(act_list, dim=0)),  # (L, dim)\n",
    "                rewards=ptu.get_numpy(torch.cat(rew_list, dim=0)),  # (L, dim)\n",
    "                terminals=np.array(term_list).reshape(-1, 1),  # (L, 1)\n",
    "                next_observations=ptu.get_numpy(\n",
    "                    torch.cat(next_obs_list, dim=0)\n",
    "                ),  # (L, dim)\n",
    "            )\n",
    "        print(\n",
    "            \"Mode:\",\n",
    "            \"Train\" if train_mode else \"Test\",\n",
    "            \"env_steps\",\n",
    "            steps,\n",
    "            \"total rewards\",\n",
    "            rewards,\n",
    "        )\n",
    "        total_steps += steps\n",
    "        total_rewards += rewards\n",
    "\n",
    "    if train_mode:\n",
    "        return total_steps\n",
    "    else:\n",
    "        return total_rewards / num_rollouts\n",
    "\n",
    "\n",
    "def update(num_updates):\n",
    "    rl_losses_agg = {}\n",
    "    # print(num_updates)\n",
    "    for update in range(num_updates):\n",
    "        # sample random RL batch: in transitions\n",
    "        batch = ptu.np_to_pytorch_batch(policy_storage.random_episodes(batch_size))\n",
    "        # RL update\n",
    "        rl_losses = agent.update(batch)\n",
    "\n",
    "        for k, v in rl_losses.items():\n",
    "            if update == 0:  # first iterate - create list\n",
    "                rl_losses_agg[k] = [v]\n",
    "            else:  # append values\n",
    "                rl_losses_agg[k].append(v)\n",
    "    # statistics\n",
    "    for k in rl_losses_agg:\n",
    "        rl_losses_agg[k] = np.mean(rl_losses_agg[k])\n",
    "    return rl_losses_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the agent: only costs < 20 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer RAM usage: 0.02 GB\n",
      "Mode: Train env_steps 200 total rewards -774.3188773952425\n",
      "Mode: Train env_steps 200 total rewards -852.4889237741008\n",
      "Mode: Train env_steps 200 total rewards -1562.3696451187134\n",
      "Mode: Train env_steps 200 total rewards -971.7877926528454\n",
      "Mode: Train env_steps 200 total rewards -877.3423518612981\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m learning_curve \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     21\u001b[0m }\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m _n_env_steps_total \u001b[38;5;241m<\u001b[39m n_env_steps_total:\n\u001b[0;32m---> 25\u001b[0m     env_steps \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rollouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_rollouts_per_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     _n_env_steps_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env_steps\n\u001b[1;32m     28\u001b[0m     train_stats \u001b[38;5;241m=\u001b[39m update(\u001b[38;5;28mint\u001b[39m(num_updates_per_iter \u001b[38;5;241m*\u001b[39m env_steps))\n",
      "File \u001b[0;32m~/miniforge3/envs/pomdp/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m, in \u001b[0;36mcollect_rollouts\u001b[0;34m(num_rollouts, random_actions, deterministic, train_mode)\u001b[0m\n\u001b[1;32m     39\u001b[0m     action \u001b[38;5;241m=\u001b[39m ptu\u001b[38;5;241m.\u001b[39mFloatTensor([env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()])  \u001b[38;5;66;03m# (1, A)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# policy takes hidden state as input for rnn, while takes obs for mlp\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     (action, _, _, _), internal_state \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_internal_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minternal_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# observe reward and next obs (B=1, dim)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m next_obs, reward, done, info \u001b[38;5;241m=\u001b[39m utl\u001b[38;5;241m.\u001b[39menv_step(env, action\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/pomdp/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/rl-spike/pomdp-baselines/policies/models/policy_rnn.py:115\u001b[0m, in \u001b[0;36mModelFreeOffPolicy_Separate_RNN.act\u001b[0;34m(self, prev_internal_state, prev_action, reward, obs, deterministic, return_log_prob)\u001b[0m\n\u001b[1;32m    112\u001b[0m reward \u001b[38;5;241m=\u001b[39m reward\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (1, B, 1)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (1, B, dim)\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m current_action_tuple, current_internal_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_internal_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_internal_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_log_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_log_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m current_action_tuple, current_internal_state\n",
      "File \u001b[0;32m~/miniforge3/envs/pomdp/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/rl-spike/pomdp-baselines/policies/models/recurrent_actor.py:195\u001b[0m, in \u001b[0;36mActor_RNN.act\u001b[0;34m(self, prev_internal_state, prev_action, reward, obs, deterministic, return_log_prob)\u001b[0m\n\u001b[1;32m    192\u001b[0m     joint_embeds \u001b[38;5;241m=\u001b[39m joint_embeds\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (B, dim)\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# 4. Actor head, generate action tuple\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m action_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoint_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_log_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_log_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action_tuple, current_internal_state\n",
      "File \u001b[0;32m~/Documents/projects/rl-spike/pomdp-baselines/policies/rl/td3.py:45\u001b[0m, in \u001b[0;36mTD3.select_action\u001b[0;34m(self, actor, observ, deterministic, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     action_tuple \u001b[38;5;241m=\u001b[39m (mean, mean, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     action \u001b[38;5;241m=\u001b[39m (mean \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexploration_noise\u001b[49m)\u001b[38;5;241m.\u001b[39mclamp(\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     47\u001b[0m     )  \u001b[38;5;66;03m# NOTE\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     action_tuple \u001b[38;5;241m=\u001b[39m (action, mean, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action_tuple\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "policy_storage = SeqReplayBuffer(\n",
    "    max_replay_buffer_size=int(buffer_size),\n",
    "    observation_dim=obs_dim,\n",
    "    action_dim=act_dim,\n",
    "    sampled_seq_len=sampled_seq_len,\n",
    "    sample_weight_baseline=0.0,\n",
    ")\n",
    "\n",
    "env_steps = collect_rollouts(\n",
    "    num_rollouts=num_init_rollouts_pool, random_actions=True, train_mode=True\n",
    ")\n",
    "_n_env_steps_total += env_steps\n",
    "\n",
    "# evaluation parameters\n",
    "last_eval_num_iters = 0\n",
    "log_interval = 5\n",
    "eval_num_rollouts = 10\n",
    "learning_curve = {\n",
    "    \"x\": [],\n",
    "    \"y\": [],\n",
    "}\n",
    "\n",
    "while _n_env_steps_total < n_env_steps_total:\n",
    "\n",
    "    env_steps = collect_rollouts(num_rollouts=num_rollouts_per_iter, train_mode=True)\n",
    "    _n_env_steps_total += env_steps\n",
    "\n",
    "    train_stats = update(int(num_updates_per_iter * env_steps))\n",
    "\n",
    "    current_num_iters = _n_env_steps_total // (\n",
    "        num_rollouts_per_iter * max_trajectory_len\n",
    "    )\n",
    "    if (\n",
    "        current_num_iters != last_eval_num_iters\n",
    "        and current_num_iters % log_interval == 0\n",
    "    ):\n",
    "        last_eval_num_iters = current_num_iters\n",
    "        average_returns = collect_rollouts(\n",
    "            num_rollouts=eval_num_rollouts,\n",
    "            train_mode=False,\n",
    "            random_actions=False,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        learning_curve[\"x\"].append(_n_env_steps_total)\n",
    "        learning_curve[\"y\"].append(average_returns)\n",
    "        print(_n_env_steps_total, average_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(learning_curve[\"x\"], learning_curve[\"y\"])\n",
    "plt.xlabel(\"env steps\")\n",
    "plt.ylabel(\"return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5f49fb63f78fde0f27b95a7c8e14eeaa9af6d816174ff450f7bbbcd21c7c97c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dyn': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
